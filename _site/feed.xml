<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Andreea Mușat</title>
    <description>No description.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Understanding EM as an MM algorithm</title>
        <description>
          
          Prerequisites Information theory concepts (entropy, cross-entropy) Optimization concepts (convex/concave functions) Jensen’s inequality MM Algorithms Optimization is virtually the center of the machine learning universe. MM algorithms (where ‘MM’ stands for ‘Minorize-Maximize’ or ‘Majorize-Minimize’) are a simple prescription for creating optimization algorithms. Let’s assume we want to find the maximizer of...
        </description>
        <pubDate>Tue, 03 Mar 2020 08:32:48 -0800</pubDate>
        <link>http://localhost:4000/2020-03-03-em/</link>
        <guid isPermaLink="true">http://localhost:4000/2020-03-03-em/</guid>
      </item>
    
      <item>
        <title>Generative methods for classification</title>
        <description>
          
          Prerequisites Basic calculus (logarithm rules, computing derivatives, properties of derivatives, chain rule for derivatives) Linear algebra (properties of traces, matrix inverses, determinants, properties of symmetric matrices) Basic probability and bayesian statistics notions (prior, posterior, Bayes’ rule, joint probability, independence, probability chain rule, Gaussian distribution, categorical distribution, most of them can...
        </description>
        <pubDate>Wed, 26 Feb 2020 14:14:48 -0800</pubDate>
        <link>http://localhost:4000/2020-02-26-generative-classification/</link>
        <guid isPermaLink="true">http://localhost:4000/2020-02-26-generative-classification/</guid>
      </item>
    
  </channel>
</rss>
